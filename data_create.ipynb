{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bartycja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Bartycja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "data_path = pathlib.Path(\"data\\korpus_wikipedia\")\n",
    "paths = list(data_path.iterdir())\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103888\n"
     ]
    }
   ],
   "source": [
    "for name in os.listdir(paths[0]):\n",
    "    if name == \"text_structure.xml\":\n",
    "        file_path = os.path.join(paths[0], name)\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        p_elements = root.findall(\".//{http://www.tei-c.org/ns/1.0}p\")\n",
    "        for p_element in p_elements:\n",
    "            try:\n",
    "                text = \"\".join(p_element.text)\n",
    "                tokenizer = nltk.data.load('tokenizers/punkt/polish.pickle')\n",
    "                sentences = tokenizer.tokenize(text)\n",
    "                for sentence in sentences:\n",
    "                    splited_sentence = sentence.split()\n",
    "                    corpus.append(splited_sentence)\n",
    "\n",
    "\n",
    "            except TypeError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 634/634 [02:59<00:00,  3.54it/s]\n"
     ]
    }
   ],
   "source": [
    "for path in tqdm(paths):\n",
    "    for name in os.listdir(path):\n",
    "        if name == \"text_structure.xml\":\n",
    "            file_path = os.path.join(path, name)\n",
    "            tree = ET.parse(file_path)\n",
    "            root = tree.getroot()\n",
    "            p_elements = root.findall(\".//{http://www.tei-c.org/ns/1.0}p\")\n",
    "            for p_element in p_elements:\n",
    "                try:\n",
    "                    text = \"\".join(p_element.text)\n",
    "                    tokenizer = nltk.data.load('tokenizers/punkt/polish.pickle')\n",
    "                    sentences = tokenizer.tokenize(text)\n",
    "                    for sentence in sentences:\n",
    "                        splited_sentence = sentence.split()\n",
    "                        corpus.append(splited_sentence)\n",
    "                        \n",
    "                except TypeError:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6310933"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AWK', 'jest', 'interpretowanym', 'językiem', 'programowania,', 'którego', 'główną', 'funkcją', 'jest', 'wyszukiwanie', 'i', 'przetwarzanie', 'wzorców.']\n",
      "['Jest', 'także', 'nazwą', 'programu', 'początkowo', 'dostępnego', 'dla', 'systemów', 'operacyjnych', 'będących', 'pochodnymi', 'UNIX-a,', 'obecnie', 'także', 'na', 'inne', 'platformy.']\n",
      "['Nazwa', 'języka', 'pochodzi', 'od', 'pierwszych', 'liter', 'nazwisk', 'jego', 'autorów', 'Alfreda', 'V.']\n",
      "['A', 'ho,', 'Petera', 'W', 'einbergera', 'i', 'Briana', 'K', 'ernighana', 'i', 'czasami', 'jest', 'zapisywana', 'małymi', 'literami', 'oraz', 'odczytywana', 'jako', 'jedno', 'słowo', 'awk', '.']\n",
      "['Definicja', 'języka', 'AWK', 'jest', 'zawarta', 'w', 'POSIX', '1003.2', 'Command', 'Language', 'And', 'Utilities', 'Standard.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('corpus_wiki.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AWK', 'jest', 'interpretowanym', 'językiem', 'programowania,', 'którego', 'główną', 'funkcją', 'jest', 'wyszukiwanie', 'i', 'przetwarzanie', 'wzorców.']\n",
      "['Jest', 'także', 'nazwą', 'programu', 'początkowo', 'dostępnego', 'dla', 'systemów', 'operacyjnych', 'będących', 'pochodnymi', 'UNIX-a,', 'obecnie', 'także', 'na', 'inne', 'platformy.']\n",
      "['Nazwa', 'języka', 'pochodzi', 'od', 'pierwszych', 'liter', 'nazwisk', 'jego', 'autorów', 'Alfreda', 'V.']\n",
      "['A', 'ho,', 'Petera', 'W', 'einbergera', 'i', 'Briana', 'K', 'ernighana', 'i', 'czasami', 'jest', 'zapisywana', 'małymi', 'literami', 'oraz', 'odczytywana', 'jako', 'jedno', 'słowo', 'awk', '.']\n",
      "['Definicja', 'języka', 'AWK', 'jest', 'zawarta', 'w', 'POSIX', '1003.2', 'Command', 'Language', 'And', 'Utilities', 'Standard.']\n"
     ]
    }
   ],
   "source": [
    "with open('corpus_wiki.pkl', 'rb') as f:\n",
    "    new_corpus = pickle.load(f)\n",
    "\n",
    "for i in range(5):\n",
    "    print(new_corpus[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
